{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MoveNet 模型\n",
    "model = hub.load('https://tfhub.dev/google/movenet/singlepose/lightning/4')\n",
    "\n",
    "# 定义输出文件夹\n",
    "output_dir = r'C:\\data\\result\\bone_recognition\\movenet'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 加载视频\n",
    "video_path = r'C:\\data\\video\\0-两手托天理三焦（八段锦）\\standard_0.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 获取视频信息\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 定义输出视频\n",
    "output_path = os.path.join(output_dir, 'output_video.avi')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义关键点\n",
    "KEYPOINTS = {\n",
    "    0: 'nose', 1: 'left_eye', 2: 'right_eye', 3: 'left_ear', 4: 'right_ear',\n",
    "    5: 'left_shoulder', 6: 'right_shoulder', 7: 'left_elbow', 8: 'right_elbow',\n",
    "    9: 'left_wrist', 10: 'right_wrist', 11: 'left_hip', 12: 'right_hip',\n",
    "    13: 'left_knee', 14: 'right_knee', 15: 'left_ankle', 16: 'right_ankle'\n",
    "}\n",
    "\n",
    "# 定义骨架连接\n",
    "EDGES = {\n",
    "    (0, 1): 'm', (0, 2): 'm', (1, 3): 'm', (2, 4): 'm', (0, 5): 'm', (0, 6): 'm',\n",
    "    (5, 7): 'm', (7, 9): 'm', (6, 8): 'm', (8, 10): 'm', (5, 6): 'y', (5, 11): 'm',\n",
    "    (6, 12): 'm', (11, 12): 'y', (11, 13): 'm', (13, 15): 'm', (12, 14): 'm', (14, 16): 'm'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频已处理并保存到 C:\\data\\result\\bone_recognition\\movenet\\output_video.avi\n"
     ]
    }
   ],
   "source": [
    "# 处理视频每一帧\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 将帧转换为 TensorFlow 需要的格式\n",
    "    input_image = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 192, 192)\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "\n",
    "    # 模型预测\n",
    "    keypoints = model.signatures['serving_default'](input_image)\n",
    "    keypoints = keypoints['output_0'].numpy()\n",
    "    \n",
    "    # 获取关键点坐标\n",
    "    keypoints = np.squeeze(keypoints)\n",
    "    \n",
    "    # 获取帧的宽和高\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # 获取鼻子和耳朵的位置\n",
    "    nose_y, nose_x, nose_confidence = keypoints[0]  # 鼻子\n",
    "    left_ear_y, left_ear_x, left_ear_confidence = keypoints[3]  # 左耳\n",
    "    right_ear_y, right_ear_x, right_ear_confidence = keypoints[4]  # 右耳\n",
    "    \n",
    "    # 获取眼睛的位置\n",
    "    left_eye_y, left_eye_x, left_eye_confidence = keypoints[1]  # 左眼\n",
    "    right_eye_y, right_eye_x, right_eye_confidence = keypoints[2]  # 右眼\n",
    "    \n",
    "    # 对眼睛位置进行限制调整\n",
    "    if left_eye_confidence > 0.3 and nose_confidence > 0.3:\n",
    "        # 限制左眼必须在鼻子上方，并在左耳的前方\n",
    "        if left_eye_y >= nose_y or left_eye_x >= left_ear_x:\n",
    "            left_eye_y = nose_y - 0.05  # 左眼在鼻子上方\n",
    "            left_eye_x = (left_ear_x + nose_x) / 2 - 0.05  # 左眼在耳朵和鼻子之间偏左\n",
    "\n",
    "    if right_eye_confidence > 0.3 and nose_confidence > 0.3:\n",
    "        # 限制右眼必须在鼻子上方，并在右耳的前方\n",
    "        if right_eye_y >= nose_y or right_eye_x <= right_ear_x:\n",
    "            right_eye_y = nose_y - 0.05  # 右眼在鼻子上方\n",
    "            right_eye_x = (right_ear_x + nose_x) / 2 + 0.05  # 右眼在耳朵和鼻子之间偏右\n",
    "    \n",
    "    # 更新关键点\n",
    "    keypoints[1] = [left_eye_y, left_eye_x, left_eye_confidence]  # 更新左眼坐标\n",
    "    keypoints[2] = [right_eye_y, right_eye_x, right_eye_confidence]  # 更新右眼坐标\n",
    "\n",
    "    # 绘制骨架\n",
    "    for edge, color in EDGES.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, confidence1 = keypoints[p1]\n",
    "        y2, x2, confidence2 = keypoints[p2]\n",
    "        if confidence1 > 0.3 and confidence2 > 0.3:\n",
    "            # 连接关键点\n",
    "            cv2.line(frame, (int(x1 * width), int(y1 * height)), (int(x2 * width), int(y2 * height)), (0, 255, 0), 2)\n",
    "    \n",
    "    # 绘制关键点\n",
    "    for idx, kp in enumerate(keypoints):\n",
    "        y, x, confidence = kp\n",
    "        if confidence > 0.3:\n",
    "            # 绘制置信度超过阈值的关键点\n",
    "            cv2.circle(frame, (int(x * width), int(y * height)), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, KEYPOINTS[idx], (int(x * width), int(y * height) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # 保存帧到输出视频\n",
    "    out.write(frame)\n",
    "    \n",
    "    # 显示处理中的视频（可选）\n",
    "    cv2.imshow('MoveNet Pose Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 释放资源\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"视频已处理并保存到 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
